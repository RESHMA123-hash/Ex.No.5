

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


# OUTPUT
## **1. Objective**

To evaluate how different prompt structures affect the **quality, accuracy, and depth** of responses generated by AI models when tested across multiple scenarios.

---

## **2. Prompt Types to Compare**

* **Broad/Unstructured Prompt:** Open-ended, vague, or loosely defined (e.g., *“Tell me about AI”*).
* **Basic/Clear/Refined Prompt:** Specific, directive, and context-rich (e.g., *“Explain the role of reinforcement learning in AI with real-world examples”*).

---

## **3. Scenarios for Testing**

Choose diverse contexts to ensure robustness:

1. **Knowledge-based question** – factual accuracy (e.g., “What are quantum computers?”).
2. **Problem-solving/analytical** – logical reasoning (e.g., “Solve a work-time problem with two workers”).
3. **Creative writing** – originality (e.g., “Write a short story about a robot in the future”).
4. **Instructional/step-based** – clarity in guidance (e.g., “Explain how to install Python on Windows step by step”).
5. **Conversational tone** – natural engagement (e.g., “I’m feeling stressed, what can I do?”).

---

## **4. Evaluation Criteria**

* **Quality of Response:**

  * Structure (clear, coherent, logically ordered).
  * Readability (easy to follow vs. confusing).

* **Accuracy:**

  * Factual correctness.
  * Relevance to the prompt.

* **Depth:**

  * Level of detail provided.
  * Coverage of different perspectives.
  * Use of examples or analogies.

---

## **5. Comparative Framework (Table Format Example)**

| Scenario                      | Prompt Type                                                                      | Response Snapshot           | Quality (1–5) | Accuracy (1–5) | Depth (1–5) | Observations          |
| ----------------------------- | -------------------------------------------------------------------------------- | --------------------------- | ------------- | -------------- | ----------- | --------------------- |
| Knowledge (Quantum Computers) | Broad: “Tell me about AI”                                                        | General overview, unfocused | 3             | 3              | 2           | Missed quantum detail |
| Knowledge (Quantum Computers) | Refined: “Explain quantum computing and how it differs from classical computing” | Clear structured answer     | 5             | 5              | 4           | Targeted, accurate    |

*(Continue for each scenario)*

---

## **6. Analysis Approach**

* **Compare Broad vs. Refined per scenario**
  → Does refining the prompt increase depth/accuracy?
* **Look for consistency across scenarios**
  → Do broad prompts always lead to vagueness?
  → Do refined prompts always improve quality, or sometimes overconstrain creativity?
* **Identify Model Behavior Patterns**
  → Some models (e.g., ChatGPT) may adapt well to vague prompts, others may struggle.

---

## **7. Expected Findings**

* Broad prompts → more generic, less accurate, weaker depth.
* Refined prompts → more structured, relevant, and deeper responses.
* Creative scenarios → broad prompts may allow more freedom.
* Instructional scenarios → refined prompts usually outperform broad ones.
  
# RESULT : The project was completed successfully within the deadline
